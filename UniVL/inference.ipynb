{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1afd874",
   "metadata": {},
   "source": [
    "# C√†i ƒë·∫∑t th∆∞ vi·ªán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64fa12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ffmpeg-python tqdm boto3 requests pandas -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6ef017",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/salaniz/pycocoevalcap.git -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56daa431",
   "metadata": {},
   "source": [
    "# T·∫£i Bert Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e21f9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd UniVL-Video-captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40b8764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed11800",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# T·∫°o th∆∞ m·ª•c cho BERT\n",
    "mkdir -p modules/bert-base-uncased\n",
    "cd modules/bert-base-uncased\n",
    "\n",
    "# Download vocab file\n",
    "wget -q https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\n",
    "mv bert-base-uncased-vocab.txt vocab.txt\n",
    "\n",
    "# Download BERT model\n",
    "wget -q https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz\n",
    "tar -xzf bert-base-uncased.tar.gz\n",
    "rm bert-base-uncased.tar.gz\n",
    "\n",
    "cd ../..\n",
    "echo \"‚úì BERT model downloaded successfully!\"\n",
    "ls -lh modules/bert-base-uncased/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852b071f",
   "metadata": {},
   "source": [
    "# T·∫£i S3D HowTo100M pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241ba8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# T·∫°o th∆∞ m·ª•c cho weights (theo c·∫•u tr√∫c VideoFeatureExtractor)\n",
    "mkdir -p VideoFeatureExtractor/model\n",
    "\n",
    "# Download S3D HowTo100M weights (~500MB)\n",
    "echo \"Downloading S3D HowTo100M weights (this may take a few minutes)...\"\n",
    "wget -q --show-progress https://www.rocq.inria.fr/cluster-willow/amiech/howto100m/s3d_howto100m.pth -O VideoFeatureExtractor/model/s3d_howto100m.pth\n",
    "\n",
    "echo \"‚úì S3D weights downloaded to VideoFeatureExtractor/model/\"\n",
    "ls -lh VideoFeatureExtractor/model/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b1cb72",
   "metadata": {},
   "source": [
    "# Extract video feature t·ª´ S3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b927674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "\n",
    "# Add VideoFeatureExtractor v√†o path (n·∫øu ch∆∞a c√≥)\n",
    "if './VideoFeatureExtractor' not in sys.path:\n",
    "    sys.path.insert(0, './VideoFeatureExtractor')\n",
    "\n",
    "def extract_s3d_features_per_second(video_path, s3d_weights_path='VideoFeatureExtractor/model/s3d_howto100m.pth', \n",
    "                                     device='cuda'):\n",
    "    \"\"\"\n",
    "    Extract S3D features v·ªõi \"1 feature per second\" (MATCHING MSRVTT training dataset)\n",
    "    \n",
    "    ‚úÖ C√ÅCH EXTRACT N√ÄY KH·ªöP V·ªöI MSRVTT TRAINING DATASET\n",
    "    \n",
    "    Logic:\n",
    "    - Decode video v·ªõi fps=16 (16 frames/second, S3D input requirement)\n",
    "    - Sliding window stride=16 frames (1 second)\n",
    "    - Video d√†i N seconds ‚Üí N features [N, 1024]\n",
    "    \n",
    "    Args:\n",
    "        video_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn video file\n",
    "        s3d_weights_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn S3D weights\n",
    "        device: 'cuda' ho·∫∑c 'cpu'\n",
    "    \n",
    "    Returns:\n",
    "        features: numpy array shape [num_seconds, 1024]\n",
    "    \"\"\"\n",
    "    # Import t·ª´ VideoFeatureExtractor\n",
    "    from VideoFeatureExtractor.video_loader import VideoLoader\n",
    "    from VideoFeatureExtractor.videocnn.models import s3dg\n",
    "    import tempfile\n",
    "    import pandas as pd\n",
    "    import ffmpeg\n",
    "    \n",
    "    print(f\"üìπ Extracting S3D features from: {video_path}\")\n",
    "    print(f\"   Method: 1 feature per second (matching MSRVTT training)\")\n",
    "    \n",
    "    # 1. Load S3D model\n",
    "    print(\"üîß Loading S3D model...\")\n",
    "    model = s3dg.S3D(last_fc=False)\n",
    "    model = model.to(device)\n",
    "    model_data = torch.load(s3d_weights_path)\n",
    "    \n",
    "    # Load weights\n",
    "    from VideoFeatureExtractor.model import init_weight\n",
    "    model = init_weight(model, model_data)\n",
    "    model.eval()\n",
    "    print(\"‚úì Model loaded\")\n",
    "    \n",
    "    # 2. Get video info\n",
    "    probe = ffmpeg.probe(video_path)\n",
    "    video_stream = next((stream for stream in probe['streams'] if stream['codec_type'] == 'video'), None)\n",
    "    \n",
    "    # Get duration in seconds\n",
    "    if 'duration' in video_stream:\n",
    "        duration = float(video_stream['duration'])\n",
    "    else:\n",
    "        duration = float(probe['format']['duration'])\n",
    "    \n",
    "    num_seconds = int(duration)\n",
    "    print(f\"üé¨ Video duration: {duration:.2f}s ‚Üí {num_seconds} seconds\")\n",
    "    \n",
    "    # 3. T·∫°o temporary CSV cho VideoLoader\n",
    "    with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv') as f:\n",
    "        temp_csv = f.name\n",
    "        df = pd.DataFrame({\n",
    "            'video_path': [video_path],\n",
    "            'feature_path': ['dummy.npy']\n",
    "        })\n",
    "        df.to_csv(temp_csv, index=False)\n",
    "    \n",
    "    try:\n",
    "        # 4. Load video v·ªõi fps=16 (16 frames/second, S3D requirement)\n",
    "        print(\"üé¨ Loading video with ffmpeg (fps=16)...\")\n",
    "        SIZE = 224\n",
    "        CENTERCROP = True\n",
    "        FPS = 16  # ‚úÖ Decode 16 frames per second (S3D requires 16-frame clips)\n",
    "        \n",
    "        dataset = VideoLoader(\n",
    "            temp_csv,\n",
    "            framerate=FPS,  # 16 fps\n",
    "            size=SIZE,\n",
    "            centercrop=CENTERCROP\n",
    "        )\n",
    "        \n",
    "        data = dataset[0]\n",
    "        video = data['video']  # [T, C, H, W] where T = duration * fps\n",
    "        \n",
    "        if video.shape[0] <= 1:\n",
    "            raise ValueError(f\"Failed to decode video: {video_path}\")\n",
    "        \n",
    "        num_decoded_frames = len(video)\n",
    "        print(f\"‚úì Loaded {num_decoded_frames} frames (fps={FPS})\")\n",
    "        \n",
    "        # 5. Extract features v·ªõi sliding window stride=16 frames (1 second)\n",
    "        print(\"üîÆ Extracting features (1 feature per second)...\")\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        video = video / 255.0\n",
    "        \n",
    "        # S·ªë clips = s·ªë gi√¢y (m·ªói gi√¢y c√≥ 16 frames ‚Üí 1 clip)\n",
    "        num_clips = num_seconds\n",
    "        features_list = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(num_clips):\n",
    "                start_frame = i * 16\n",
    "                end_frame = start_frame + 16\n",
    "                \n",
    "                # Get 16 frames cho clip n√†y\n",
    "                if end_frame <= num_decoded_frames:\n",
    "                    clip = video[start_frame:end_frame]  # [16, C, H, W]\n",
    "                else:\n",
    "                    # Pad n·∫øu kh√¥ng ƒë·ªß 16 frames\n",
    "                    clip = video[start_frame:]\n",
    "                    padding_needed = 16 - len(clip)\n",
    "                    padding = torch.zeros(padding_needed, clip.shape[1], clip.shape[2], clip.shape[3])\n",
    "                    clip = torch.cat([clip, padding], dim=0)\n",
    "                \n",
    "                # Reshape to [1, C, T, H, W] for S3D input\n",
    "                clip = clip.permute(1, 0, 2, 3).unsqueeze(0)  # [1, 3, 16, 224, 224]\n",
    "                clip = clip.to(device)\n",
    "                \n",
    "                # Extract feature\n",
    "                feature = model(clip)  # [1, 1024]\n",
    "                feature = F.normalize(feature, dim=1)  # L2 normalize\n",
    "                features_list.append(feature.cpu())\n",
    "        \n",
    "        # Concat all features\n",
    "        features = torch.cat(features_list, dim=0).numpy()  # [num_seconds, 1024]\n",
    "        \n",
    "        print(f\"‚úì Features extracted! Shape: {features.shape}\")\n",
    "        print(f\"   {features.shape[0]} features (1 feature per second)\")\n",
    "        \n",
    "        return features\n",
    "        \n",
    "    finally:\n",
    "        # Cleanup\n",
    "        if os.path.exists(temp_csv):\n",
    "            os.unlink(temp_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c363d86f",
   "metadata": {},
   "source": [
    "# Load model UniVL t·ª´ checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2227f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from modules.tokenization import BertTokenizer\n",
    "from modules.modeling import UniVL\n",
    "from modules.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n",
    "\n",
    "import argparse\n",
    "\n",
    "# T·∫°o parser gi·ªëng nh∆∞ trong main_task_caption.py\n",
    "parser = argparse.ArgumentParser(description='UniVL Inference')\n",
    "\n",
    "# Required arguments v·ªõi default values (ƒë·ªß ƒë·ªÉ kh·ªüi t·∫°o model)\n",
    "parser.add_argument(\"--bert_model\", default=\"bert-base-uncased\", type=str)\n",
    "parser.add_argument(\"--visual_model\", default=\"visual-base\", type=str)\n",
    "parser.add_argument(\"--cross_model\", default=\"cross-base\", type=str)\n",
    "parser.add_argument(\"--decoder_model\", default=\"decoder-base\", type=str)\n",
    "parser.add_argument(\"--do_lower_case\", action='store_true', default=True)\n",
    "parser.add_argument(\"--cache_dir\", default=\"\", type=str)\n",
    "\n",
    "# Model architecture params\n",
    "parser.add_argument('--max_words', type=int, default=48)\n",
    "parser.add_argument('--max_frames', type=int, default=48)\n",
    "parser.add_argument('--video_dim', type=int, default=1024)\n",
    "parser.add_argument('--text_num_hidden_layers', type=int, default=12)\n",
    "parser.add_argument('--visual_num_hidden_layers', type=int, default=6)\n",
    "parser.add_argument('--cross_num_hidden_layers', type=int, default=2)\n",
    "parser.add_argument('--decoder_num_hidden_layers', type=int, default=3)\n",
    "\n",
    "# Task config\n",
    "parser.add_argument('--stage_two', action='store_true', default=True)\n",
    "parser.add_argument('--task_type', type=str, default='caption')\n",
    "parser.add_argument(\"--do_pretrain\", action='store_true', default=False)  # Inference mode\n",
    "parser.add_argument(\"--do_train\", action='store_true', default=False)\n",
    "parser.add_argument(\"--do_eval\", action='store_true', default=False)\n",
    "\n",
    "# Loss-related params (c·∫ßn cho kh·ªüi t·∫°o model)\n",
    "parser.add_argument('--batch_size', type=int, default=32)\n",
    "parser.add_argument('--n_gpu', type=int, default=1)\n",
    "parser.add_argument('--n_pair', type=int, default=1)\n",
    "parser.add_argument('--margin', type=float, default=0.1)\n",
    "parser.add_argument('--negative_weighting', type=int, default=1)\n",
    "parser.add_argument('--hard_negative_rate', type=float, default=0.5)\n",
    "parser.add_argument('--use_mil', action='store_true', default=False)\n",
    "parser.add_argument('--sampled_use_mil', action='store_true', default=False)\n",
    "\n",
    "# Distributed training\n",
    "parser.add_argument('--local_rank', type=int, default=0)\n",
    "parser.add_argument('--world_size', type=int, default=1)\n",
    "\n",
    "# Parse v·ªõi empty args (s·ª≠ d·ª•ng t·∫•t c·∫£ default values)\n",
    "args = parser.parse_args([])\n",
    "\n",
    "print(\"‚úì Arguments initialized with default values from training script\")\n",
    "\n",
    "# Load tokenizer\n",
    "print(\"\\nüîß Loading tokenizer...\")\n",
    "tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)\n",
    "print(f\"‚úì Tokenizer loaded (vocab size: {len(tokenizer.vocab)})\")\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint_path = \"/kaggle/input/best-univl/pytorch_model.bin\"\n",
    "print(f\"\\nüéØ Loading checkpoint from: {checkpoint_path}\")\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"   Device: {device}\")\n",
    "    \n",
    "    # Load model\n",
    "    model_state_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    cache_dir = args.cache_dir if args.cache_dir else os.path.join(str(PYTORCH_PRETRAINED_BERT_CACHE), 'distributed')\n",
    "    \n",
    "    model = UniVL.from_pretrained(\n",
    "        args.bert_model, args.visual_model, args.cross_model, args.decoder_model,\n",
    "        cache_dir=cache_dir, state_dict=model_state_dict, task_config=args\n",
    "    )\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"‚úì Model loaded and ready for inference!\")\n",
    "    print(f\"   Parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")\n",
    "    print(\"\\nüìù Using argparse with default values (same as training script)\")\n",
    "    print(\"   This ensures all required attributes are present!\")\n",
    "else:\n",
    "    print(f\"‚ùå Checkpoint not found: {checkpoint_path}\")\n",
    "    print(\"\\nüí° TIP: ƒê·∫£m b·∫£o file checkpoint t·ªìn t·∫°i v√† ƒë∆∞·ªùng d·∫´n ƒë√∫ng.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2473cb0",
   "metadata": {},
   "source": [
    "# H√†m inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c330375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from modules.beam import Beam\n",
    "\n",
    "# ‚úÖ Helper functions for beam search (copied from main_task_caption.py to avoid distributed init issues)\n",
    "\n",
    "def get_inst_idx_to_tensor_position_map(inst_idx_list):\n",
    "    \"\"\"Get the positional map of active instances in the batch.\"\"\"\n",
    "    return {inst_idx: tensor_position for tensor_position, inst_idx in enumerate(inst_idx_list)}\n",
    "\n",
    "def collect_active_part(beamed_tensor, curr_active_inst_idx, n_prev_active_inst, n_bm):\n",
    "    \"\"\"Collect tensor parts associated with active instances.\"\"\"\n",
    "    _, *d_hs = beamed_tensor.size()\n",
    "    n_curr_active_inst = len(curr_active_inst_idx)\n",
    "    new_shape = (n_curr_active_inst * n_bm, *d_hs)\n",
    "\n",
    "    beamed_tensor = beamed_tensor.view(n_prev_active_inst, -1)\n",
    "    beamed_tensor = beamed_tensor.index_select(0, curr_active_inst_idx)\n",
    "    beamed_tensor = beamed_tensor.view(*new_shape)\n",
    "\n",
    "    return beamed_tensor\n",
    "\n",
    "def collate_active_info(input_tuples, inst_idx_to_position_map, active_inst_idx_list, n_bm, device):\n",
    "    \"\"\"Collate active beam instances.\"\"\"\n",
    "    assert isinstance(input_tuples, tuple)\n",
    "    sequence_output_rpt, visual_output_rpt, input_ids_rpt, input_mask_rpt, video_mask_rpt = input_tuples\n",
    "\n",
    "    n_prev_active_inst = len(inst_idx_to_position_map)\n",
    "    active_inst_idx = [inst_idx_to_position_map[k] for k in active_inst_idx_list]\n",
    "    active_inst_idx = torch.LongTensor(active_inst_idx).to(device)\n",
    "\n",
    "    active_sequence_output_rpt = collect_active_part(sequence_output_rpt, active_inst_idx, n_prev_active_inst, n_bm)\n",
    "    active_visual_output_rpt = collect_active_part(visual_output_rpt, active_inst_idx, n_prev_active_inst, n_bm)\n",
    "    active_input_ids_rpt = collect_active_part(input_ids_rpt, active_inst_idx, n_prev_active_inst, n_bm)\n",
    "    active_input_mask_rpt = collect_active_part(input_mask_rpt, active_inst_idx, n_prev_active_inst, n_bm)\n",
    "    active_video_mask_rpt = collect_active_part(video_mask_rpt, active_inst_idx, n_prev_active_inst, n_bm)\n",
    "    active_inst_idx_to_position_map = get_inst_idx_to_tensor_position_map(active_inst_idx_list)\n",
    "\n",
    "    return (active_sequence_output_rpt, active_visual_output_rpt, active_input_ids_rpt, \n",
    "            active_input_mask_rpt, active_video_mask_rpt), active_inst_idx_to_position_map\n",
    "\n",
    "def beam_decode_step(decoder, inst_dec_beams, len_dec_seq, inst_idx_to_position_map, n_bm, device, input_tuples):\n",
    "    \"\"\"Decode and update beam status.\"\"\"\n",
    "    assert isinstance(input_tuples, tuple)\n",
    "\n",
    "    def prepare_beam_dec_seq(inst_dec_beams, len_dec_seq):\n",
    "        dec_partial_seq = [b.get_current_state() for b in inst_dec_beams if not b.done]\n",
    "        dec_partial_seq = torch.stack(dec_partial_seq).to(device)\n",
    "        dec_partial_seq = dec_partial_seq.view(-1, len_dec_seq)\n",
    "        return dec_partial_seq\n",
    "\n",
    "    def predict_word(next_decoder_ids, n_active_inst, n_bm, device, input_tuples):\n",
    "        sequence_output_rpt, visual_output_rpt, input_ids_rpt, input_mask_rpt, video_mask_rpt = input_tuples\n",
    "        next_decoder_mask = torch.ones(next_decoder_ids.size(), dtype=torch.uint8).to(device)\n",
    "\n",
    "        dec_output = decoder(sequence_output_rpt, visual_output_rpt, input_ids_rpt, input_mask_rpt,\n",
    "                             video_mask_rpt, next_decoder_ids, next_decoder_mask, shaped=True, get_logits=True)\n",
    "        dec_output = dec_output[:, -1, :]\n",
    "        word_prob = torch.nn.functional.log_softmax(dec_output, dim=1)\n",
    "        word_prob = word_prob.view(n_active_inst, n_bm, -1)\n",
    "        return word_prob\n",
    "\n",
    "    def collect_active_inst_idx_list(inst_beams, word_prob, inst_idx_to_position_map):\n",
    "        active_inst_idx_list = []\n",
    "        for inst_idx, inst_position in inst_idx_to_position_map.items():\n",
    "            is_inst_complete = inst_beams[inst_idx].advance(word_prob[inst_position])\n",
    "            if not is_inst_complete:\n",
    "                active_inst_idx_list += [inst_idx]\n",
    "        return active_inst_idx_list\n",
    "\n",
    "    n_active_inst = len(inst_idx_to_position_map)\n",
    "    dec_seq = prepare_beam_dec_seq(inst_dec_beams, len_dec_seq)\n",
    "    word_prob = predict_word(dec_seq, n_active_inst, n_bm, device, input_tuples)\n",
    "\n",
    "    active_inst_idx_list = collect_active_inst_idx_list(inst_dec_beams, word_prob, inst_idx_to_position_map)\n",
    "\n",
    "    return active_inst_idx_list\n",
    "\n",
    "def collect_hypothesis_and_scores(inst_dec_beams, n_best):\n",
    "    \"\"\"Collect final hypothesis and scores.\"\"\"\n",
    "    all_hyp, all_scores = [], []\n",
    "    for inst_idx in range(len(inst_dec_beams)):\n",
    "        scores, tail_idxs = inst_dec_beams[inst_idx].sort_scores()\n",
    "        all_scores += [scores[:n_best]]\n",
    "        hyps = [inst_dec_beams[inst_idx].get_hypothesis(i) for i in tail_idxs[:n_best]]\n",
    "        all_hyp += [hyps]\n",
    "    return all_hyp, all_scores\n",
    "\n",
    "# Main inference function\n",
    "def generate_caption_from_features(video_features, model, tokenizer, device='cuda', \n",
    "                                   max_frames=48, beam_size=5, max_words=48):\n",
    "    \"\"\"\n",
    "    Generate caption t·ª´ video features\n",
    "    \n",
    "    ‚úÖ Logic copied 100% t·ª´ eval_epoch() trong main_task_caption.py (d√≤ng 560-640)\n",
    "    \n",
    "    Args:\n",
    "        video_features: numpy array [num_frames, 1024]\n",
    "        model: Trained UniVL model\n",
    "        tokenizer: BERT tokenizer\n",
    "        device: 'cuda' ho·∫∑c 'cpu'\n",
    "        max_frames: Max frames (48)\n",
    "        beam_size: Beam search size (5)\n",
    "        max_words: Max caption length (48)\n",
    "    \n",
    "    Returns:\n",
    "        caption: Generated caption string\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Preprocess features\n",
    "    num_frames = video_features.shape[0]\n",
    "    \n",
    "    # Truncate ho·∫∑c pad v·ªÅ max_frames=48\n",
    "    if num_frames > max_frames:\n",
    "        video_features = video_features[:max_frames]\n",
    "        num_frames = max_frames\n",
    "    else:\n",
    "        padding = np.zeros((max_frames - num_frames, 1024), dtype=np.float32)\n",
    "        video_features = np.concatenate([video_features, padding], axis=0)\n",
    "    \n",
    "    # Create mask\n",
    "    video_mask = np.zeros(max_frames, dtype=np.int64)\n",
    "    video_mask[:num_frames] = 1\n",
    "    \n",
    "    # Convert to tensors (batch size = 1)\n",
    "    video = torch.from_numpy(video_features).unsqueeze(0).float().to(device)  # [1, 48, 1024]\n",
    "    video_mask = torch.from_numpy(video_mask).unsqueeze(0).long().to(device)  # [1, 48]\n",
    "    \n",
    "    # ‚úÖ T·∫°o dummy input theo ƒë√∫ng source (line 574-576)\n",
    "    input_ids = torch.zeros(1, 1, dtype=torch.long).to(device)\n",
    "    input_ids[0, 0] = tokenizer.vocab[\"[CLS]\"]\n",
    "    input_mask = torch.ones(1, 1, dtype=torch.long).to(device)\n",
    "    segment_ids = torch.zeros(1, 1, dtype=torch.long).to(device)  # ‚úÖ C·∫ßn segment_ids!\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # ‚úÖ D√πng get_sequence_visual_output() nh∆∞ source (line 579)\n",
    "        sequence_output, visual_output = model.get_sequence_visual_output(\n",
    "            input_ids, segment_ids, input_mask, video, video_mask\n",
    "        )\n",
    "        \n",
    "        # ‚úÖ Beam search logic (copied t·ª´ line 580-620)\n",
    "        n_bm = beam_size\n",
    "        device = sequence_output.device\n",
    "        n_inst, len_s, d_h = sequence_output.size()\n",
    "        _, len_v, v_h = visual_output.size()\n",
    "        \n",
    "        decoder = model.decoder_caption\n",
    "        \n",
    "        # Note: shaped first (line 587-589)\n",
    "        input_ids = input_ids.view(-1, input_ids.shape[-1])\n",
    "        input_mask = input_mask.view(-1, input_mask.shape[-1])\n",
    "        video_mask = video_mask.view(-1, video_mask.shape[-1])\n",
    "        \n",
    "        # Repeat for beam (line 591-595)\n",
    "        sequence_output_rpt = sequence_output.repeat(1, n_bm, 1).view(n_inst * n_bm, len_s, d_h)\n",
    "        visual_output_rpt = visual_output.repeat(1, n_bm, 1).view(n_inst * n_bm, len_v, v_h)\n",
    "        input_ids_rpt = input_ids.repeat(1, n_bm).view(n_inst * n_bm, len_s)\n",
    "        input_mask_rpt = input_mask.repeat(1, n_bm).view(n_inst * n_bm, len_s)\n",
    "        video_mask_rpt = video_mask.repeat(1, n_bm).view(n_inst * n_bm, len_v)\n",
    "        \n",
    "        # Prepare beams (line 597-601)\n",
    "        inst_dec_beams = [Beam(n_bm, device=device, tokenizer=tokenizer) for _ in range(n_inst)]\n",
    "        active_inst_idx_list = list(range(n_inst))\n",
    "        inst_idx_to_position_map = get_inst_idx_to_tensor_position_map(active_inst_idx_list)\n",
    "        \n",
    "        # Decode (line 602-613)\n",
    "        for len_dec_seq in range(1, max_words + 1):\n",
    "            active_inst_idx_list = beam_decode_step(\n",
    "                decoder, inst_dec_beams, len_dec_seq, inst_idx_to_position_map, n_bm, device,\n",
    "                (sequence_output_rpt, visual_output_rpt, input_ids_rpt, input_mask_rpt, video_mask_rpt)\n",
    "            )\n",
    "            \n",
    "            if not active_inst_idx_list:\n",
    "                break\n",
    "            \n",
    "            (sequence_output_rpt, visual_output_rpt, input_ids_rpt, input_mask_rpt, video_mask_rpt), \\\n",
    "            inst_idx_to_position_map = collate_active_info(\n",
    "                (sequence_output_rpt, visual_output_rpt, input_ids_rpt, input_mask_rpt, video_mask_rpt),\n",
    "                inst_idx_to_position_map, active_inst_idx_list, n_bm, device\n",
    "            )\n",
    "        \n",
    "        # Collect results (line 615-616)\n",
    "        batch_hyp, batch_scores = collect_hypothesis_and_scores(inst_dec_beams, 1)\n",
    "        result_list = [batch_hyp[i][0] for i in range(n_inst)]\n",
    "        \n",
    "        # Decode to text (line 620-634)\n",
    "        for re_list in result_list:\n",
    "            decode_text_list = tokenizer.convert_ids_to_tokens(re_list)\n",
    "            if \"[SEP]\" in decode_text_list:\n",
    "                SEP_index = decode_text_list.index(\"[SEP]\")\n",
    "                decode_text_list = decode_text_list[:SEP_index]\n",
    "            if \"[PAD]\" in decode_text_list:\n",
    "                PAD_index = decode_text_list.index(\"[PAD]\")\n",
    "                decode_text_list = decode_text_list[:PAD_index]\n",
    "            decode_text = ' '.join(decode_text_list)\n",
    "            decode_text = decode_text.replace(\" ##\", \"\").strip(\"##\").strip()\n",
    "            \n",
    "            return decode_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f521a9",
   "metadata": {},
   "source": [
    "# Video to Caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28582494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_to_caption(video_path, model, tokenizer, \n",
    "                     s3d_weights_path='VideoFeatureExtractor/model/s3d_howto100m.pth',\n",
    "                     beam_size=5, verbose=True):\n",
    "    \"\"\"\n",
    "    End-to-End pipeline: Video ‚Üí Caption\n",
    "    \n",
    "    Args:\n",
    "        video_path: Path to video file\n",
    "        model: Trained UniVL model\n",
    "        tokenizer: BERT tokenizer\n",
    "        s3d_weights_path: Path to S3D HowTo100M weights\n",
    "        device: 'cuda' or 'cpu'\n",
    "        beam_size: Beam search size (2-5)\n",
    "        verbose: Print progress\n",
    "    \n",
    "    Returns:\n",
    "        caption: Generated caption string\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"üé¨ Processing video: {video_path}\")\n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    # Step 1: Extract S3D features\n",
    "    if verbose:\n",
    "        print(\"\\nüìπ Step 1/2: Extracting S3D features...\")\n",
    "    \n",
    "    features = extract_s3d_features_per_second(\n",
    "        video_path=video_path,\n",
    "        s3d_weights_path='VideoFeatureExtractor/model/s3d_howto100m.pth',\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        batch_size=8\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"   ‚úì Features extracted: {features.shape}\")\n",
    "    \n",
    "    # Step 2: Generate caption\n",
    "    if verbose:\n",
    "        print(\"\\nüí¨ Step 2/2: Generating caption...\")\n",
    "    \n",
    "    caption = generate_caption_from_features(\n",
    "        video_features=features,\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        max_frames=48,\n",
    "        beam_size=beam_size,\n",
    "        max_words=48\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"   ‚úì Caption generated!\")\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"üìù Caption: {caption}\")\n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030283ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"/kaggle/input/video-captioning-dataset/Video_Datasets/Video_Datasets/Test/video7010.mp4\"\n",
    "\n",
    "caption = video_to_caption(\n",
    "    video_path=video_path,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    beam_size=5\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
